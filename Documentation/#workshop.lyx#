#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 1.5cm
\rightmargin 1.5cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
workshop - Anomaly Detection and Characterization in time series
\end_layout

\begin_layout Author
Adi Berger - 301044657, Idan Attias, Gilad Rubin
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Anomaly detection in time series is a challenging problem with numerous
 potential applications.
 A comprehensive anomaly detection approach not only should be able to detect
 and identify the emerging anomalies but also to characterize the essence
 of these anomalie in a way that is understandable to the end-user as well.
 Our goal is to create a generic tool which according to a business KPI
 implements a comprehensive anomaly detection and visualize the results
 for the end user.
 the KPI is the attribute which the business wish to analzyze during time.
 Thus, we sought to produce a generic tool which will work on every data
 set that composed of events over time, with the given KPI attribute and
 the features collected on that event.
\begin_inset Newline newline
\end_inset

Based on our data exploration process, we have discovered that many anomalies
 are caused by some underlying process which affects only a particular subset
 of the data.
 For example: if we look at the Request Duration attribute over time splitted
 by the RoleInst attribute:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset Graphics
	filename role inst.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

It is clear to see that the subset of data with attribue RoleInst = RD000D3A002C
F6 is a potential lead for the anomaly seen in the plot.
 Our goal is to find the most informative leads for anomalies in a time
 series that will make the Anomaly Detection and Inspection more accurate
 and efficient.
 
\begin_inset Newline newline
\end_inset

In order to detect the underlying causes for the anomalies seen in the data,
 we implemented a model which consists of two main steps: we first use an
 anomaly detector on the whole dataset to identify individual records with
 anomalous attribute values (the KPI attribute), secondly, we detect and
 rank leads for the underlying causes for the given anomolous records.
 We carry out this method by performing an exhaustive search over many subsets
 of the data in order to find the most probable common denominator for the
 anomalies.
 We wish to detect any such subset of data which displays a significant
 increase in anomalous activity as compared to the normal behavior of the
 system.
 We perform significance testing to determine if any subset of the data
 is significantly different than expected.
\begin_inset Newline newline
\end_inset

To evaluate the proposed method, we synthesized data and investigated the
 results.
 
\end_layout

\begin_layout Section
Model workflow
\end_layout

\begin_layout Subsection
The workflow of our model is depicted below:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename diagram.png
	lyxscale 50
	scale 50

\end_inset


\begin_inset Newline newline
\end_inset

In order to analyze a given dataset, each record in it must contain a Timestamp,
 a KPI attribute (numeric value) and categorical attributes related to that
 event.
\end_layout

\begin_layout Subsection
With the abovementioned generic dataset, we aggregate the events into a
 pre-defined time window, calculate the average of the KPI attribute in
 each window and create a time series based on these values.
 
\end_layout

\begin_layout Subsection
In order to choose the optimal time-window size, we implemented an algorithm
 which searches through different time windows and other parameters related
 to the outlier detection algorithm and used the values which gave the highest
 f-score using a labeled synthesized dataset.
 
\begin_inset Newline newline
\end_inset

With the previously selected optimal parameters, we insert the time series
 into an anomaly detection algorithm, based on STL decomposition (See ......)
 and present the detected anomalies to the end-user, highlighted in colors
 according to their severety.
 
\end_layout

\begin_layout Subsection
Following the anomaly detection procedure, we treat each anomaly seperately
 by taking all the combinations of attribute-values that exist in the anomaly
 time window, limited to a combination of up to 3 attributes.
 for instance if the anomaly window contains the records: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename demonstrate for workshop.emf
	scale 10

\end_inset


\begin_inset Newline newline
\end_inset

The following 1-tuple combinations can be: (attribute 1 = 2); (attribute
 1 = 10); (attribute 2 = 1); (attribute 2 = 5) etc...
\begin_inset Newline newline
\end_inset

With a combination of k-tuples, we perform a cartesian product between the
 distinct values of each feature of the k-features, for example, with k=3:
 (attribute 1 = 2, attribute 2 = 1, attribute 3 = 7); (attribute 1 =2, attribute
 2 = 5, attribute 3 = 4) etc...
\end_layout

\begin_layout Subsection
Given a tuple a specific anomaly, we create a new data set by eliminating
 the records containing this tuple from the original data set.
 then, we aggregate the data set as explained before and run the same anomaly
 detection on the filtered dataset.
 Our hypothesis is that if the algorithm doesn't show an anomaly on the
 same point as the inspected anomaly - we have a strong indication that
 this tuple may be an informative lead for the anomaly.
 We have confirmed this hypothesis via our tests (See .......).
 
\end_layout

\begin_layout Subsection
At this stage, we have an indication for each tuple whether its corresponding
 filtered time series had an anomolous event at the inspected time window.
 If the filtered time series still contains an anomaly at the inspected
 time window - we can infer that this tuple is not a good lead to finding
 the cause for that anomaly.
 However, in the opposite case, we believe that the end-user should be able
 to inspect this tuple as a lead to the cause for the anomaly.
\end_layout

\begin_layout Subsection
Because there might be several anomalies that we believe are potential leads,
 we rank and sort them using 3 measures attained from our algorithm:
\end_layout

\begin_layout Subsubsection
\begin_inset Quotes eld
\end_inset

New Distance
\begin_inset Quotes erd
\end_inset

 score - at the anomaly point, what is the absolute distance in standard
 deviations between the expected value generated in the anomaly detection
 algorithm and the actual value of the time series at that point.
 The lower the score, the stronger indication that this tuple caused the
 anomaly.
\end_layout

\begin_layout Subsubsection
\begin_inset Quotes eld
\end_inset

Unique Anomalies
\begin_inset Quotes erd
\end_inset

 - the number of anomalies found in the filtered time series that didn't
 appear in the original time series.
 We use this metric in order to as an efficient indicator for the dissimalirity
 between the two time series.
 The lower the number, the higher the probability that the two time series
 are similar.
\end_layout

\begin_layout Subsubsection
\begin_inset Quotes eld
\end_inset

Overall Anomalies
\begin_inset Quotes erd
\end_inset

 - the percentage of anomalies that the filtered time series agrees with
 the original time series.
 This is another indication of the similarity between the time series, but
 for lower numbers - it can tell us that the inspected lead is a potential
 cause for many anomalies in other time points.
\end_layout

\begin_layout Subsection
After having the scores for all the tuples that show no anomaly at the specified
 time, we cluster them according to both metrics (after standartization):
 
\begin_inset Quotes eld
\end_inset

New Distance
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Unique Anomalies
\begin_inset Quotes erd
\end_inset

.
 After some experiments we concluded that combining these two features as
 the main ranking scheme will provide a good estimate of the effect each
 tuple had on the anomaly.
\end_layout

\begin_layout Subsection
We sort the clusters by their proximity to the origin (0,0), beacuse the
 lower both scores are the greater the probability for the tuple to be a
 good lead for the anomaly.
 
\end_layout

\begin_layout Subsection
Finally, we present to the end user a list which consist of all that tuples,
 ordered by the cluster distance from the origin and with a secondary sorting
 using the third score: 
\begin_inset Quotes eld
\end_inset

Overall Anomalies
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Section
Data Preparation Process
\end_layout

\begin_layout Standard
We chose to focus on the Request Duaration KPI, as seen in the 
\begin_inset Quotes eld
\end_inset

requests
\begin_inset Quotes erd
\end_inset

 datasets provided by Microsoft.
 The data preprocessing can be summarized in two steps:
\end_layout

\begin_layout Subsection
Select and Preprocess data
\end_layout

\begin_layout Standard
We picked the relevant 
\begin_inset Quotes eld
\end_inset

requests
\begin_inset Quotes erd
\end_inset

 files and merge them to one csv file with all the information, filtering
 out columns that had full correlation to other column and columns that
 were very sparse (more than 80% missing values).
 We ended up with a large data set (~700K rows), where each row represents
 a request event with request duration, time stamp and some more attributes.
 Furthermore, we filtered out record with timestamps after 2015-10-07 18:00,
 since these records had insufficient amount of data points for our time-series
 analysis.
\end_layout

\begin_layout Subsection
Transform Data
\end_layout

\begin_layout Standard
As explained in the Model Workflow section, the main transformation applied
 on our datasets is aggregation by a defined time-window.
 In order to find the optimal time-window - we ran a search over tuples
 of parameters with changing time-windows and picked the best performing
 parameters, according to the f-score derived from comparing the detected
 anomalies to the labeles examples in a synthesised dataset.
 
\begin_inset Newline newline
\end_inset

In a real world application, we would have to manually define what considers
 as an anomaly in our dataset, since there can be many interpretations of
 what is an anomaly.
 Our implementation includes a process which recieves a dataset and an array
 of anomaly times and selects the optimal parameters for this dataset.
\end_layout

\begin_layout Section
Anomaly detection
\end_layout

\begin_layout Subsection
ARIMA-based Anomaly Detection
\end_layout

\begin_layout Standard
Since our workflow involves detection of anomalies in a very large amount
 of time series, our goal was to find an algorithm for anomaly detection
 which combines good accuracy and fast performance times.
 In our search for this kind of algorithm, we have tested an anomaly detection
 process based on modeling an ARIMA model with 2 different fourier terms
 (to account for the daily and weekly seasonality) and finding data points
 which are located outside of the confidence interval generated by the model.
 We also tested the function 
\begin_inset Quotes eld
\end_inset

tso
\begin_inset Quotes erd
\end_inset

 in the package 
\begin_inset Quotes eld
\end_inset

tsoutliers
\begin_inset Quotes erd
\end_inset

, that also performs an iterated version of the ARIMA fitting process by
 removing outliers in the first stage, re-fitting of the model and removal
 of the next outliers until it reaches a stopping criterion.
 
\begin_inset Newline newline
\end_inset

These two methods were tested on our datasets and although they showed good
 accuracy in detection of anomalies - their performance measurments showed
 that they were inadequate for this type of workflow.
 
\end_layout

\begin_layout Subsection
STL Decomposition Anomaly Detection
\end_layout

\begin_layout Standard
In contrast to the abovementiond methods, there exists a very accurate and
 efficient algorithm, based on STL Decomposition, found in the 
\begin_inset Quotes eld
\end_inset

forecast
\begin_inset Quotes erd
\end_inset

 package in R (tsoutliers).
\begin_inset Newline newline
\end_inset

The algorithm performs a seasonality adjustment on the time series using
 an STL decomposition and smoothing of the Trend element using Friedman's
 
\begin_inset Quotes eld
\end_inset

Super Smoother
\begin_inset Quotes erd
\end_inset

 Algorithm.
 It subtracts the 
\begin_inset Quotes eld
\end_inset

smoothed
\begin_inset Quotes erd
\end_inset

 trend from the seasonally adjusted time series and tests this new time
 series of residuals for deviations from a normal distribution, based on
 its Interquartile Ranges.
 A demonstration of this process on the 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 dataset is depicted via the following figures:
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
STL Decomposition on 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 dataset, showing the original series, the seasonal element, trend and residuals
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL-Smooth.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Seasonally-Adjusted 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 time series with the smoothed Trend line over it as the expected value
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL Residuals.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Residuals from the 
\begin_inset Quotes eld
\end_inset

expected value
\begin_inset Quotes erd
\end_inset

 (Smooth trend line) and the Seasonally-Adjusted time series.
 It is clear that the residuals conform to a normal distribution with mean
 0, with some outliers outside the third quartile of the distribution
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Detected Anomalies.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Plot of the Requests time series, aggregated by a 5-min time window with
 the anomalies highlighted by their severity
\end_layout

\end_inset


\end_layout

\begin_layout Section
Anomaly characterization
\end_layout

\begin_layout Subsection
Detecting Potential Leads for Anomalies
\end_layout

\begin_layout Standard
Throughout the process of considering different business cases to address
 in our project, we performed a survey on anomaly detection algorithms.
 We discovered that there are many processes that assist in detecting anomalies
 in time series data, but we could not find any method for pointing out
 possible causes for the anomalies seen in the data.
\begin_inset Newline newline
\end_inset

For this reason, we have decided to design a workflow that its main goal
 is to channel analysts efforts into the most probable causes of each anomaly,
 by providing a ranked list of probable leads.
 In order to provide this information in an accurate and informative manner,
 we decided to focus our attention on expanding the search space for potential
 leads, at the cost of performance times.
\begin_inset Newline newline
\end_inset

We have tested 3 different approaches for this task:
\end_layout

\begin_layout Subsubsection
Regression Tree
\end_layout

\begin_layout Standard
Given a time window labeled as an anomaly, we were able to fit a regression
 tree on the values of residuals for each record in the dataset.
 The output of the model was a tree for which each node represents a tuples
 of features along with their mean residual value.
 Its main strength is in its simplicity and efficiency - it has the ability
 to combine huge categorical variables into a short set of interpertable
 rules.
 Furthermore, it implicitly inclues variable selection and in-feature grouping
 of values.
 We used those properties and the fact that regression trees is relatively
 fast, even with large data sets, for optimization purposes.
 
\begin_inset Newline newline
\end_inset

However, this model relies on the assumption that the higher the residuals
 - the higher the probabilty of being a potential lead.
 This may not be the case in various time series.
 
\begin_inset Newline newline
\end_inset

Consider the situation where the request duration times for US residents
 is usually lower than average and the request duration times of Russian
 residents is usually higher than average.
 If at a certain time window the servers which attend to US residents fail
 to provide fast response times - their request time will increase, but
 perhaps not above the average duration of Russian residents.
 In this case, there will be an anomaly in the time series and the Regression
 Tree will be biased towards emphasizing tuples of features including Russian
 residents instead of US residents.
 
\begin_inset Newline newline
\end_inset

For this reason, we have decided to test a different method, which relies
 on the finding anomalies based on the change in distance between subsets
 of the populations compared to the original time series without the subset's
 records.
\end_layout

\begin_layout Subsubsection
P-Value
\end_layout

\begin_layout Standard
For an anomaly we take all the combinations of values that exist in the
 anomaly time window.
 for each such tuple we extracted distances vector.
 distances vector represents the distance of all the request duration (KPI
 attribute) of the selected subset events, from the average of all the events
 excluding this subset, at each point this subset had an event.
 meaning, how far the events of that subset are usually from all the rest
 of the data.
 this vector is produced for all the events that accured before the anomaly
 we wish to explore.
 for this new vector of samples we fitted a distribution, and estimated
 according to the recieved distribution the p-value of the distance at the
 anomaly point.
 the distance at the anomaly point calculated as the distance from the subset
 event request duration to the predicitive value received from the STL algorithm.
 the p-value score gives strong indication if this subset actually characterize
 the anomaly.
 because the lower the value is the lower the probability for the given
 distance.
 which means we can assume that this subset chracterize the anomaly and
 we should take it under considuration for representing it to the end user.
\begin_inset Newline newline
\end_inset

You can learn more about the implementation of this procedure in the appendix
\end_layout

\begin_layout Subsubsection
winning approach
\end_layout

\begin_layout Standard
because the emphasis of our model is on characterize the anomalies with
 high accuracy relatively faster, optimization for subsets selection..
 easy to understand.
 accuracy nore important because we emphasis on characterization we decided
 to analyze all the subsets 
\begin_inset Newline newline
\end_inset

parallelize
\end_layout

\begin_layout Subsection
Score subsets
\end_layout

\begin_layout Subsection
Cluster subsets
\end_layout

\begin_layout Standard
We used the Ward's method to clusetr the subsets by their score.
 The aim in Ward’s method is to join cases into clusters such that the variance
 within a cluster is minimised.
 To do this, each case begins as its own cluster.
 Clusters are then merged in such a way as to reduce the variability within
 a cluster.
 To be more precise, two clusters are merged if this merger results in the
 minimum increase in the error sum of squares.
 Basically, this means that at each stage the average similarity of the
 cluster is measured.
 The difference between each cases within a cluster and that average similarity
 is calculated and squared (just like calculating a standard deviation).
 The sum of squared deviations is used as a measure of error within a cluster.
 A case is selected to enter the cluster if it is the case whose inclusion
 in the cluster produces the least increase in the error (as measured by
 the sum of squared deviations).
 No apriori information about the number of clusters required.
 We choose the closest cluster to the origin and take the subsets which
 correspond to the cluster cases as mentioned in section 2.
\end_layout

\begin_layout Subsection
Rank subsets
\end_layout

\begin_layout Section
Synthetic Data Set
\end_layout

\begin_layout Standard
The goal is to create a supervised data set for illustrating and testing
 our model.
 We aimed to synthesize data which preserves our data properties in terms
 of seasonality and trend.
 But, we didn't want to synthesize a very simillar data set, because, our
 goal was to create a generic method.
 So, the creation of points around that seasonality and trend, didn't rely
 on the original data set events charasteristic (as their standard deviation
 around the mean etc.).
 
\end_layout

\begin_layout Subsection
creating data set
\end_layout

\begin_layout Standard
First of all, to acheive the main properties about our data, we trimmed
 the events with request duration value greater than 
\begin_inset Formula $\pm$
\end_inset

3 standard deviations from the mean.
 In order to create seasonality and trend, we calculated the average request
 duration per day and per hour.
 Furthermore we calculated the ratio of the average request duration of
 an hour and average request duration of a day, assuming that this ratio
 is similiar for each hour and day (based on the seasonality we have seen
 in our data).
 For each hour we created approximately 4000 (number of events per hour
 on average) random time stamps, then attached to it request duration values
 that were generated from normal distribution with proper mean and standard
 deviation for this hour.
 We added to this data frame another 4 columns which represents our attributes.
 Each coulmn values were generated from different discrete distributions.
 The choice of discrete distributions is owing to the fact that our original
 features are categorical.
 1 attribute was chosen from discrete uniform distribution, 2 attributes
 from (different) binomial distributions and another attribute from poisson
 distribution.
 
\end_layout

\begin_layout Subsection
insert anomalies
\end_layout

\begin_layout Standard
At this point we want to inject artificial anomalies in random time stamps,
 and randomly choose how many features and which features to be the cause
 of the anomaly.
 In addition, we generated randomly a value which is the strength of the
 anomaly.
 That way, we will have different kind of anomalies (global and seasonal).
 For each event in the anomaly time range that contains the 
\begin_inset Quotes eld
\end_inset

problematic
\begin_inset Quotes erd
\end_inset

 features, we increase the request duration- add to this value the multiplicatio
n of the strengh of the anomaly with the standard deviation that was calculated
 the previous stage.
 We recored the properties of each anomaly so we can measure our result
 on the synthetic data.
 In the end, we export our synthetic data frame to csv file.
 Illustration of the synthetic data with anomalies:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename 100_5-15.png
	scale 70

\end_inset


\end_layout

\begin_layout Section
Testing
\end_layout

\begin_layout Subsection
Anomaly detection accuracy
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Subsection
p-value calculation
\end_layout

\begin_layout Subsubsection
Fitting 
\series bold
Distribution to data sample:
\end_layout

\begin_layout Standard
\noindent
Fitting distributions consists in finding a mathematical function which
 represents in a good way a statistical variable.
 For some observations of a quantitative character 
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

 we wishes to test if those observations, being a sample of an unknown populatio
n, belong from a population with a pdf (probability density function) f(x,θ),
 where θ is a vector of parameters to estimate from available data.
 
\begin_inset Newline newline
\end_inset

We can identify 3 steps in fitting distributions:
\end_layout

\begin_layout Enumerate
Model/function choice: hypothesize families of distributions; 
\end_layout

\begin_layout Enumerate
Estimate parameters;
\end_layout

\begin_layout Enumerate
Goodness of fit statistical tests.
\end_layout

\begin_layout Subsubsection
Model choice:
\end_layout

\begin_layout Standard
On the paper Probabilistic approaches to risk by Aswath Damodaran.
 In Appendix 6.1 Aswath discusses the key characteristics of the most common
 distributions and in Figure 6A.15 he provides us with a decision tree diagram
 for choosing a distribution:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename distributions.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

Our statistic representing the distance from the predictive request duration,
 thus, is continuos 'not only positive and there are no limits on the data.
 therefoe, we will choose all the continuos distrubutions except the Exponential
 and the Triangular as our hypothesize families of distributions.
\end_layout

\begin_layout Subsubsection
Estimate parameters:
\end_layout

\begin_layout Standard
After choosing the models that can mathematically represent our data we
 have to estimate parameters of such model.
 There are several estimate methods in statistical literature, we focused,
 as we sdudied in statistic course, on maximum likelihood:
\begin_inset Newline newline
\end_inset

We have a random variable with a known pdf f(x,θ) describing a quantitative
 character in the population.
 We should estimate the vector of costant and unknown parameters θ according
 to sampling data: 
\series bold

\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset


\series default
.
 
\begin_inset Newline newline
\end_inset

Maximum likelihood estimation begins with the mathematical expression known
 as a likelihood function of the sample data.
 Loosely speaking, the likelihood of a set of data is the probability of
 obtaining that particular set of data given the chosen probability model.
 This expression contains the unknown parameters.
 Those values of the parameter that maximize the sample likelihood are known
 as the maximum likelihood estimates (MLE).
 
\begin_inset Newline newline
\end_inset

We define the likelihood function as: 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)=\prod_{i=1}^{n}f(x_{i},\vartheta)$
\end_inset

 MLE consist in finding θ which maximizes 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)$
\end_inset

 or its logarithmic function.
\begin_inset Newline newline
\end_inset

We can employ mathematical analysis methods (partial derivates equal to
 zero) when the likelihood function is rather simple, but very often we
 should had using iterative methods.
 therefore, we implemented using R method fitdistr() - included in package
 MASS for maximum-likelihood fitting of univariate distributions without
 any information about likelihood analytical expression.
 It is enough to specify a data vector, the type of pdf and eventually the
 list of starting values for iterative procedure.
\end_layout

\begin_layout Subsubsection
Goodness of fit tests:
\end_layout

\begin_layout Standard
Goodness of fit tests indicate whether or not it is reasonable to assume
 that a random sample comes from a specific distribution.
 They are a form of hypothesis testing where the null and alternative hypotheses
 are:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{0}$
\end_inset

: Sample data come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{A}$
\end_inset

: Sample data do not come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

These tests are sometimes called as omnibus test and they are distribution
 free, meaning they do not depend according the pdf.
\begin_inset Newline newline
\end_inset

We chose using the Kolmogorov-Smirnov test:
\begin_inset Newline newline
\end_inset

This test is used to decide if a sample comes from a population with a specific
 distribution.
\begin_inset Newline newline
\end_inset

It is restricted to continuous distributions and based on a comparison between
 the empirical distribution function (ECDF) and the theoretical one defined
 as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F(x)=\intop_{a}^{x}f(y,\vartheta)dy$
\end_inset

 , where f(y,θ) is the pdf.
\begin_inset Newline newline
\end_inset

Given n ordered data points 
\begin_inset Formula $x_{1},x_{2},...x_{n}$
\end_inset

 , the ECDF is defined as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F_{n}(X_{i})=N(i)/n$
\end_inset

 where N(i) is the number of points less than 
\begin_inset Formula $X_{i}$
\end_inset

(
\begin_inset Formula $X_{i}$
\end_inset

are ordered from smallest to largest value).
 This is a step function that increases by 1/n at the value of each ordered
 data point.
\begin_inset Newline newline
\end_inset

The test statistic used is:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $D_{n}=sup_{1\leq i\leq n}|F(X_{i})-F_{n}(X_{i})|$
\end_inset

 that is the upper extreme among absolute value differencies between ECDF
 and theoretical CDF.
\begin_inset Newline newline
\end_inset

The hypothesis regarding the distributional form is rejected if the test
 statistic, 
\begin_inset Formula $D_{n}$
\end_inset

, is greater than the critical value obtained from a table, or, which is
 the same, if the p-value is lower than the significance level.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Another test usually taken into consideration is the chi-square goodness
 of fit test.
 But it is applied to binned data, namely data that have been split into
 classes.
 Unfortunately, this test depends consistently on how the data have been
 binned.
\begin_inset Newline newline
\end_inset

The Kolmogorov-Smirnov test is more powerful than the chi-square test in
 the cases in which the sample size is, so to say, unreliable by the presence
 of noise, missing values, etc.
 For large sample sizes both the tests have the same power.
 One limitation of the Kolmogorov-Smirnov test is that the distribution
 needs to be fully specified.
 Therefore the shape and its estimated parameters must be computed beforehand.
 That is exactly what our code does.
\end_layout

\begin_layout Subsubsection
R procedure
\end_layout

\begin_layout Standard
The procedure parameters are the data (distances samples) and the distance
 of the request duration of a specific features subset event from the predictive
 request duration at the anomaly point .
 The purpose of the procedure is to select the best shape and the best parameter
s for the distributions mentioned above and perform a significance test
 of that selection as explained.
 The goal here cannot be to determine with certainty what distribution our
 sample follows.
 The goal is what calls parsimonious approximate descriptions of the data.
 We measure the P-Value of the given distance according to the distditribution
 which explain our data most.
 and return it for determine if to use that subset in the next stage.
 (the procedure is in find_dist.R)
\end_layout

\end_body
\end_document
