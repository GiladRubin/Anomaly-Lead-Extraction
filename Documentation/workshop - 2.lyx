#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 1.5cm
\rightmargin 1.5cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
workshop - Anomaly Detection and Characterization in time series
\end_layout

\begin_layout Author
Adi Berger - 301044657, Idan Attias, Gilad Rubin
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Anomaly detection in time series is a challenging problem with numerous
 potential applications.
 A comprehensive anomaly detection approach not only should be able to detect
 and identify the emerging anomalies but also to characterize the essence
 of these anomalie in a way that is understandable to the end-user as well.
 Our goal is to create a generic tool which according to a business KPI
 implements a comprehensive anomaly detection and visualize the results
 for the end user.
 the KPI is the attribute which the business wish to analzyze during time.
 Thus, we sought to produce a generic tool which will work on every data
 set that composed of events over time, with the given KPI attribute and
 the features collected on that event.
\begin_inset Newline newline
\end_inset

Based on our data exploration process, we have discovered that many anomalies
 are caused by some underlying process which affects only a particular subset
 of the data.
 For example: if we look at the Request Duration attribute over time splitted
 by the RoleInst attribute:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

 
\begin_inset Graphics
	filename role inst.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

It is clear to see that the subset of data with attribue RoleInst = RD000D3A002C
F6 is a potential lead for the anomaly seen in the plot.
 
\begin_inset Newline newline
\end_inset

In order to detect the underlying causes for the anomalies seen in the data,
 we implemente a model which consists of two main steps: we first use an
 anomaly detector on the whole dataset to identify individual records with
 anomalous attribute values (the KPI attribute), secondly, we detect and
 rank leads for the underlying causes for the given anomolous records.
 We carry out this method by performing an exhaustive search over many subsets
 of the data in order to find the most probable common denominator for the
 anomalies.
 We wish to detect any such subset of data which displays a significant
 increase in anomalous activity as compared to the normal behavior of the
 system.
 We perform significance testing to determine if any subset of the data
 is significantly different than expected.
\begin_inset Newline newline
\end_inset

To evaluate the proposed method, we synthesized data and investigated the
 results.
 
\end_layout

\begin_layout Section
Model workflow
\end_layout

\begin_layout Standard
The workflow of our model is depicted below:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename diagram.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\begin_layout Standard
\noindent
The passed data set to analyze has to be composed from events over time.
 Each event has to consist the KPI attribute (continuous vlaues), a time
 stamp attribute and categorical attributes related to that event.
 The anomaly detection algorithms we used works on aggregated data.
 which means, divide the data according to a time window, calculate the
 average of the KPI attribute in each window and create a new data set composed
 of time series and the aggregated values.
 To decide on the time window size, we used an algorithm which optimize
 the time window value for supervised dataset.
 To achieve that, we used our synthetic supervised data set.
 Hence, for the input data set we aggregate it by the time window we've
 got.
 We passed that new data set to the STL decomposition algorithm for anomaly
 detection.
 In that phase, the application end-user can see the graph with the anomalies
 highlited in colors according to its severe.
 Now, after finding the anomalies we suggesting the option to choose an
 anomaly by date for analysis.
 
\begin_inset Newline newline
\end_inset

Per each anomaly we take all the combinations of values that exist in the
 anomaly time window.
 up to 3 values.
 for instance if the anomaly window contains the the records: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename demonstrate for workshop.emf
	lyxscale 10
	scale 10

\end_inset


\begin_inset Newline newline
\end_inset

,off course that in real life a combination usually repeat itself in a lot
 of records, than a combination can be up to 3 values where each value from
 different attribute.
 for example (2,9) from attributes 1 and 4, and (10, 5, 4) from attributes
 1,2, 3 and so on..
\begin_inset Newline newline
\end_inset

Now, for each tuple we create new data set by reducing the records containing
 this tuple from the main data set.
 then, we aggregate the data set as explained before and run the STL algorithm.
 Intuitively, if the algorithm doesn't return anomaly on the same point
 the anomaly was before the tuple reduction, we have a srong indication
 that this tuple may caused the anomaly.
 From the algorithm results we get the anomalies and the predictive values
 for each point on the time series.
 If on the new data set the anomaly remains, significantly different from
 the expected value by confidance level of 95%.
 it means that this tuple wasn't the cause for that anomaly.
 For the suspiciuos tuples (the ones that could cause the anomaly) we use
 the results to measure 3 different scores that supply informative data
 on those tuples:
\end_layout

\begin_layout Enumerate
New distance - at the anomaly point, the new distance from the expected
 value in standard deviation (the lower it is the stronger indication this
 tuple caused the anomaly)
\end_layout

\begin_layout Enumerate
Unique anomalies - how much anomalies found that didn't appear on the former
 data set (which indicates ...)
\end_layout

\begin_layout Enumerate
Overall anomalies = 1- The percentage of common anomalies for both data
 sets.
 which means, the percentage of anomalies this tuple removed from the origin
 data set.
 that gives an indication if this tuple is problematic, causing a lot of
 anomalies.
 (maybe false positive, maybe allready known problem...).
 make it feasible to carry out a review of all the historical events related
 to that context of the data
\end_layout

\begin_layout Standard
After having the scores for all the remainig tuples, we cluster them according
 to scores New distane and Unique anomalies.
 After some experiments we figuerd out that only the first score is not
 enough because if the tuple records are a major portion of the data.
 the new time series is not similar to the old one and the fact that there
 is no anomaly on that point in the new time series can result from the
 fact that a big number of records were removed.
 so using the second score is to solve just that.
 From the clustered tuples we take the closest cluster to the origin.
 beacuse the lower both scores are the greater percentage the tuple explain
 the anomaly.
 We present to the end user a list consist from all that tuples ordered
 by the third score overall anomalies.
\end_layout

\begin_layout Section
Data Preparation Process
\end_layout

\begin_layout Standard
We chose to focus on the request duaration KPI.
 The data preprocessing can be summarized in two steps:
\end_layout

\begin_layout Subsection
Select and Preprocess data
\end_layout

\begin_layout Standard
We've noticed there is a lot of data available which contains information
 about request duration, our chosen KPI.
 We picked the relevant files and merge them to one csv file with all the
 information.
\begin_inset Newline newline
\end_inset

We filtered out columns that had full corelation to other column (where
 the remain column explian the other) and columns that were very sparse
 (more than 80% missing values).
 We ended up with a large data set (~700K rows), each row represents a request
 event with request duration, time stamp and some more attributes.
 In this created data set we replaced the nan values with Null value, gave
 the correct type for each attribute: date time for timestamp, float for
 request duration and categorical to the rest attributes.
 Furthermore, the data was noncontinuous from the date: 2015-10-07 18:00.
 and due to our on the continuity and seasonality of the data 
\end_layout

\begin_layout Subsection
Transform Data
\end_layout

\begin_layout Standard
As explained in the above section.
 we divided the data according to the time window received from the algorithm
 we ran on the synthetic data (add algorithm explanation..), we calculated
 the average of the KPI attribute in each window and create a new data set
 composed of time series and the aggregated values
\end_layout

\begin_layout Section
Anomaly detection
\end_layout

\begin_layout Subsection
Approaches:
\end_layout

\begin_layout Standard
we implemented 3 different algorithms for anomaly detection...
\end_layout

\begin_layout Subsubsection
TSO
\end_layout

\begin_layout Standard
....
 blabla.r with the code
\end_layout

\begin_layout Subsubsection
Arima
\end_layout

\begin_layout Subsubsection
STL
\end_layout

\begin_layout Subsection
comparison and which one we chose
\end_layout

\begin_layout Standard
pros and cons/ trade offs of those approaches and which one we chose to
 our model and why
\end_layout

\begin_layout Section
Anomaly characterization
\end_layout

\begin_layout Subsection
Finding suspicious subsets
\end_layout

\begin_layout Standard
we tried 3 different approaches, explained below.
 
\end_layout

\begin_layout Subsubsection
New time series
\end_layout

\begin_layout Subsubsection
P-Value
\end_layout

\begin_layout Standard
For an anomaly we take all the combinations of values that exist in the
 anomaly time window.
 for each such tuple we extracted distances vector.
 distances vector represents the distance of all the request duration (KPI
 attribute) of the selected subset events, from the average of all the events
 excluding this subset, at each point this subset had an event.
 meaning, how far the events of that subset are usually from all the rest
 of the data.
 this vector is produced for all the events that accured before the anomaly
 we wish to explore.
 for this new vector of samples we fitted a distribution, and estimated
 according to the recieved distribution the p-value of the distance at the
 anomaly point.
 the distance at the anomaly point calculated as the distance from the subset
 event request duration to the predicitive value received from the STL algorithm.
 the p-value score gives strong indication if this subset actually characterize
 the anomaly.
 because the lower the value is the lower the probability for the given
 distance.
 which means we can assume that this subset chracterize the anomaly and
 we should take it under considuration for representing it to the end user.
\begin_inset Newline newline
\end_inset

You can learn more about the implementation of this procedure in the appendix
\end_layout

\begin_layout Subsubsection
Regression tree classifier
\end_layout

\begin_layout Standard
For an anomaly, we take the predictive value received from the STL algorithm
 (explained above).
 For each tuple that exist in the anomaly time window, we calculated the
 distance in standard deviation (received also by the algorithm) from the
 predictive value.
 We used the regression tree algorithm for clustering those tuples according
 to that distance and limiteded the tree depth to level 4.
 Regression tree do not require the assumptions of statistical models and
 has the ability to automatically bin massively categorical variables into
 a few categories.
 Hence, variable selection & reduction is done automaticly.
 We used those properties and the fact that regression trees is relatively
 fast, even with large data sets, for optimization.
 Instead of anlayze the whole features values combinations, We looked only
 at the values combinations created by the tree.
 when a combination is not only the nodes on a path from root to a leaf.
 but also all the subgroups of this path (from root to node).
 Because, those values explain in a good way the distribution of the tuples
 around the anomaly.
 this approach improves the efficiency of the model but lower the accuracy
\end_layout

\begin_layout Subsubsection
winning approach
\end_layout

\begin_layout Standard
because the emphasis of our model is on characterize the anomalies with
 high accuracy
\end_layout

\begin_layout Standard
relatively fasr, optimization for subsets selection..
 easy to understand.
 accuracy nore important because we emphasis on characterization we decided
 to analyze all the subsets 
\end_layout

\begin_layout Subsection
Score subsets
\end_layout

\begin_layout Subsection
Cluster subsets
\end_layout

\begin_layout Standard
We used the Ward's method to clusetr the subsets by their score.
 The aim in Ward’s method is to join cases into clusters such that the variance
 within a cluster is minimised.
 To do this, each case begins as its own cluster.
 Clusters are then merged in such a way as to reduce the variability within
 a cluster.
 To be more precise, two clusters are merged if this merger results in the
 minimum increase in the error sum of squares.
 Basically, this means that at each stage the average similarity of the
 cluster is measured.
 The difference between each cases within a cluster and that average similarity
 is calculated and squared (just like calculating a standard deviation).
 The sum of squared deviations is used as a measure of error within a cluster.
 A case is selected to enter the cluster if it is the case whose inclusion
 in the cluster produces the least increase in the error (as measured by
 the sum of squared deviations).
 No apriori information about the number of clusters required.
 We choose the closest cluster to the origin and take the subsets which
 correspond to the cluster cases as mentioned in section 2.
\end_layout

\begin_layout Subsection
Rank subsets
\end_layout

\begin_layout Section
Synthetic Data Set
\end_layout

\begin_layout Standard
The goal is to create a supervised data set for illustrating and testing
 our model.
 We aimed to synthesize data which preserves our data properties in terms
 of seasonality and trend.
 But, we didn't want to synthesize a very simillar data set, because, our
 goal was to create a generic method.
 So, the creation of points around that seasonality and trend, didn't rely
 on the original data set events charasteristic (as their standard deviation
 around the mean etc.).
 
\end_layout

\begin_layout Subsection
creating data set
\end_layout

\begin_layout Standard
First of all, to acheive the main properties about our data, we trimmed
 the events with request duration value greater than 
\begin_inset Formula $\pm$
\end_inset

3 standard deviations from the mean.
 In order to create seasonality and trend, we calculated the average request
 duration per day and per hour.
 Furthermore we calculated the ratio of the average request duration of
 an hour and average request duration of a day, assuming that this ratio
 is similiar for each hour and day (based on the seasonality we have seen
 in our data).
 For each hour we created approximately 4000 (number of events per hour
 on average) random time stamps, then attached to it request duration values
 that were generated from normal distribution with proper mean and standard
 deviation for this hour.
 We added to this data frame another 4 columns which represents our attributes.
 Each coulmn values were generated from different discrete distributions.
 The choice of discrete distributions is owing to the fact that our original
 features are categorical.
 1 attribute was chosen from discrete uniform distribution, 2 attributes
 from (different) binomial distributions and another attribute from poisson
 distribution.
 
\end_layout

\begin_layout Subsection
insert anomalies
\end_layout

\begin_layout Standard
At this point we want to inject artificial anomalies in random time stamps,
 and randomly choose how many features and which features to be the cause
 of the anomaly.
 In addition, we generated randomly a value which is the strength of the
 anomaly.
 That way, we will have different kind of anomalies (global and seasonal).
 For each event in the anomaly time range that contains the 
\begin_inset Quotes eld
\end_inset

problematic
\begin_inset Quotes erd
\end_inset

 features, we increase the request duration- add to this value the multiplicatio
n of the strengh of the anomaly with the standard deviation that was calculated
 the previous stage.
 We recored the properties of each anomaly so we can measure our result
 on the synthetic data.
 In the end, we export our synthetic data frame to csv file.
 Illustration of the synthetic data with anomalies:
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename 100_5-15.png
	scale 70

\end_inset


\end_layout

\begin_layout Section
Testing
\end_layout

\begin_layout Subsection
Anomaly detection accuracy
\end_layout

\begin_layout Standard
We measured the accuracy of the anomaly detection process using F-score.
 It considers both the precision and the recall and can be interpreted as
 the harmonic mean of this two scores.
 F-score reaches its best value at 1 and worst at 0.
 
\begin_inset Newline newline
\end_inset

To explain our calculation we first define the following parameters:
\begin_inset Newline newline
\end_inset

tp (true positive) = #anomalies correctly labeled as anomalies 
\begin_inset Newline newline
\end_inset

fp (false positive) = #anomalies incorrectly labled as anomalies 
\begin_inset Newline newline
\end_inset

fn (false negative) = #anomalies that were not labeled as anomalies
\begin_inset Newline newline
\end_inset

Precision = 
\begin_inset Formula $\frac{tp}{tp+fp}$
\end_inset

 , meaning, from all the anomalies our model detected what is the percentage
 we were right
\begin_inset Newline newline
\end_inset

Recall = 
\begin_inset Formula $\frac{tp}{tp+fn}$
\end_inset

 , meaning from all the anomalies actuall exist in the data what is the
 prcentage that we detected
\begin_inset Newline newline
\end_inset

F-score = 
\begin_inset Formula $2*\frac{precision*recall}{precision+recall}$
\end_inset


\end_layout

\begin_layout Subsection
Leads revealed accuracy
\end_layout

\begin_layout Standard
as mentioned in section 5, the output of our model for every detected anomaly
 are the leads for that anomaly ranked by overall anomalies score.
 the accuracy for this process is measured as the precentage of anomalies
 the right tuple apeared as a result.
 Furthermore we measures the accuracy regarding the top k leads.
 Demonstration to out precision is depicted below.
 
\end_layout

\begin_layout Section
Conclusions
\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Subsection
p-value calculation
\end_layout

\begin_layout Subsubsection
Fitting 
\series bold
Distribution to data sample:
\end_layout

\begin_layout Standard
\noindent
Fitting distributions consists in finding a mathematical function which
 represents in a good way a statistical variable.
 For some observations of a quantitative character 
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

 we wishes to test if those observations, being a sample of an unknown populatio
n, belong from a population with a pdf (probability density function) f(x,θ),
 where θ is a vector of parameters to estimate from available data.
 
\begin_inset Newline newline
\end_inset

We can identify 3 steps in fitting distributions:
\end_layout

\begin_layout Enumerate
Model/function choice: hypothesize families of distributions; 
\end_layout

\begin_layout Enumerate
Estimate parameters;
\end_layout

\begin_layout Enumerate
Goodness of fit statistical tests.
\end_layout

\begin_layout Subsubsection
Model choice:
\end_layout

\begin_layout Standard
On the paper Probabilistic approaches to risk by Aswath Damodaran.
 In Appendix 6.1 Aswath discusses the key characteristics of the most common
 distributions and in Figure 6A.15 he provides us with a decision tree diagram
 for choosing a distribution:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename distributions.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

Our statistic representing the distance from the predictive request duration,
 thus, is continuos 'not only positive and there are no limits on the data.
 therefoe, we will choose all the continuos distrubutions except the Exponential
 and the Triangular as our hypothesize families of distributions.
\end_layout

\begin_layout Subsubsection
Estimate parameters:
\end_layout

\begin_layout Standard
After choosing the models that can mathematically represent our data we
 have to estimate parameters of such model.
 There are several estimate methods in statistical literature, we focused,
 as we sdudied in statistic course, on maximum likelihood:
\begin_inset Newline newline
\end_inset

We have a random variable with a known pdf f(x,θ) describing a quantitative
 character in the population.
 We should estimate the vector of costant and unknown parameters θ according
 to sampling data: 
\series bold

\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset


\series default
.
 
\begin_inset Newline newline
\end_inset

Maximum likelihood estimation begins with the mathematical expression known
 as a likelihood function of the sample data.
 Loosely speaking, the likelihood of a set of data is the probability of
 obtaining that particular set of data given the chosen probability model.
 This expression contains the unknown parameters.
 Those values of the parameter that maximize the sample likelihood are known
 as the maximum likelihood estimates (MLE).
 
\begin_inset Newline newline
\end_inset

We define the likelihood function as: 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)=\prod_{i=1}^{n}f(x_{i},\vartheta)$
\end_inset

 MLE consist in finding θ which maximizes 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)$
\end_inset

 or its logarithmic function.
\begin_inset Newline newline
\end_inset

We can employ mathematical analysis methods (partial derivates equal to
 zero) when the likelihood function is rather simple, but very often we
 should had using iterative methods.
 therefore, we implemented using R method fitdistr() - included in package
 MASS for maximum-likelihood fitting of univariate distributions without
 any information about likelihood analytical expression.
 It is enough to specify a data vector, the type of pdf and eventually the
 list of starting values for iterative procedure.
\end_layout

\begin_layout Subsubsection
Goodness of fit tests:
\end_layout

\begin_layout Standard
Goodness of fit tests indicate whether or not it is reasonable to assume
 that a random sample comes from a specific distribution.
 They are a form of hypothesis testing where the null and alternative hypotheses
 are:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{0}$
\end_inset

: Sample data come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{A}$
\end_inset

: Sample data do not come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

These tests are sometimes called as omnibus test and they are distribution
 free, meaning they do not depend according the pdf.
\begin_inset Newline newline
\end_inset

We chose using the Kolmogorov-Smirnov test:
\begin_inset Newline newline
\end_inset

This test is used to decide if a sample comes from a population with a specific
 distribution.
\begin_inset Newline newline
\end_inset

It is restricted to continuous distributions and based on a comparison between
 the empirical distribution function (ECDF) and the theoretical one defined
 as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F(x)=\intop_{a}^{x}f(y,\vartheta)dy$
\end_inset

 , where f(y,θ) is the pdf.
\begin_inset Newline newline
\end_inset

Given n ordered data points 
\begin_inset Formula $x_{1},x_{2},...x_{n}$
\end_inset

 , the ECDF is defined as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F_{n}(X_{i})=N(i)/n$
\end_inset

 where N(i) is the number of points less than 
\begin_inset Formula $X_{i}$
\end_inset

(
\begin_inset Formula $X_{i}$
\end_inset

are ordered from smallest to largest value).
 This is a step function that increases by 1/n at the value of each ordered
 data point.
\begin_inset Newline newline
\end_inset

The test statistic used is:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $D_{n}=sup_{1\leq i\leq n}|F(X_{i})-F_{n}(X_{i})|$
\end_inset

 that is the upper extreme among absolute value differencies between ECDF
 and theoretical CDF.
\begin_inset Newline newline
\end_inset

The hypothesis regarding the distributional form is rejected if the test
 statistic, 
\begin_inset Formula $D_{n}$
\end_inset

, is greater than the critical value obtained from a table, or, which is
 the same, if the p-value is lower than the significance level.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Another test usually taken into consideration is the chi-square goodness
 of fit test.
 But it is applied to binned data, namely data that have been split into
 classes.
 Unfortunately, this test depends consistently on how the data have been
 binned.
\begin_inset Newline newline
\end_inset

The Kolmogorov-Smirnov test is more powerful than the chi-square test in
 the cases in which the sample size is, so to say, unreliable by the presence
 of noise, missing values, etc.
 For large sample sizes both the tests have the same power.
 One limitation of the Kolmogorov-Smirnov test is that the distribution
 needs to be fully specified.
 Therefore the shape and its estimated parameters must be computed beforehand.
 That is exactly what our code does.
\end_layout

\begin_layout Subsubsection
R procedure
\end_layout

\begin_layout Standard
The procedure parameters are the data (distances samples) and the distance
 of the request duration of a specific features subset event from the predictive
 request duration at the anomaly point .
 The purpose of the procedure is to select the best shape and the best parameter
s for the distributions mentioned above and perform a significance test
 of that selection as explained.
 The goal here cannot be to determine with certainty what distribution our
 sample follows.
 The goal is what calls parsimonious approximate descriptions of the data.
 We measure the P-Value of the given distance according to the distditribution
 which explain our data most.
 and return it for determine if to use that subset in the next stage.
 (the procedure is in find_dist.R)
\end_layout

\end_body
\end_document
