#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\begin_modules
theorems-ams
eqs-within-sections
figs-within-sections
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1.5cm
\topmargin 1.5cm
\rightmargin 1.5cm
\bottommargin 1.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
workshop - Anomaly Detection and Characterization in time series
\end_layout

\begin_layout Author
Adi Berger - 301044657, Idan Attias, Gilad Rubin
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Anomaly detection in time series is a challenging problem with numerous
 potential applications.
 A comprehensive anomaly detection approach not only should be able to detect
 and identify the emerging anomalies but also to characterize the essence
 of these anomalie in a way that is understandable to the end-user as well.
 Our goal is to create a generic tool which according to a business KPI
 implements a comprehensive anomaly detection and visualize the results
 for the end user.
 the KPI is the attribute which the business wish to analzyze during time.
 Thus, we sought to produce a generic tool which will work on every data
 set that composed of events over time, with the given KPI attribute and
 the features collected on that event.
\begin_inset Newline newline
\end_inset

Based on our data exploration process, we have discovered that many anomalies
 are caused by some underlying process which affects only a particular subset
 of the data.
 For example: if we look at the Request Duration attribute over time splitted
 by the RoleInst attribute:
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename role.png

\end_inset


\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

It is clear to see that the subset of data with attribue RoleInst = RD000D3A002C
F6 is a potential lead for the anomaly seen in the plot.
 Our goal is to find the most informative leads for anomalies in a time
 series that will make the Anomaly Detection and Inspection more accurate
 and efficient.
 
\begin_inset Newline newline
\end_inset

In order to detect the underlying causes for the anomalies seen in the data,
 we implemented a model which consists of two main steps: we first use an
 anomaly detector on the whole dataset to identify individual records with
 anomalous attribute values (the KPI attribute), secondly, we detect and
 rank leads for the underlying causes for the given anomolous records.
 We carry out this method by performing an exhaustive search over many subsets
 of the data in order to find the most probable common denominator for the
 anomalies.
 We wish to detect any such subset of data which displays a significant
 increase in anomalous activity as compared to the normal behavior of the
 system.
 We perform significance testing to determine if any subset of the data
 is significantly different than expected.
\begin_inset Newline newline
\end_inset

To evaluate the proposed method, we synthesized data and investigated the
 results.
 
\end_layout

\begin_layout Section
Model workflow
\end_layout

\begin_layout Subsection
The workflow of our model is depicted below:
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename flow.png
	lyxscale 50
	scale 50

\end_inset


\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

In order to analyze a given dataset, each record in it must contain a Timestamp,
 a KPI attribute (numeric value) and categorical attributes 
\end_layout

\begin_layout Subsection
With the abovementioned generic dataset, we aggregate the events into a
 pre-defined time window, calculate the average of the KPI attribute in
 each window and create a time series based on these values.
 
\end_layout

\begin_layout Subsection
In order to choose the optimal time-window size, we implemented an algorithm
 which searches through different time windows and other parameters related
 to the outlier detection algorithm and used the values which gave the highest
 f-score using a labeled synthesized dataset.
 
\begin_inset Newline newline
\end_inset

With the previously selected optimal parameters, we insert the time series
 into an anomaly detection algorithm, based on STL decomposition (See ......)
 and present the detected anomalies to the end-user, highlighted in colors
 according to their severety.
 
\end_layout

\begin_layout Subsection
Following the anomaly detection procedure, we treat each anomaly seperately
 by taking all the combinations of attribute-values that exist in the anomaly
 time window, limited to a combination of up to 3 attributes.
 for instance if the anomaly window contains the records: 
\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename demonstrate for workshop.emf
	lyxscale 20
	scale 10

\end_inset


\begin_inset Newline newline
\end_inset

The following 1-tuple combinations can be: (attribute 1 = 2); (attribute
 1 = 10); (attribute 2 = 1); (attribute 2 = 5) etc...
\begin_inset Newline newline
\end_inset

With a combination of k-tuples, we perform a cartesian product between the
 distinct values of each feature of the k-features, for example, with k=3:
 (attribute 1 = 2, attribute 2 = 1, attribute 3 = 7); (attribute 1 =2, attribute
 2 = 5, attribute 3 = 4) etc...
\end_layout

\begin_layout Subsection
Given a tuple a specific anomaly, we create a new data set by eliminating
 the records containing this tuple from the original data set.
 then, we aggregate the data set as explained before and run the same anomaly
 detection on the filtered dataset.
 Our hypothesis is that if the algorithm doesn't show an anomaly on the
 same point as the inspected anomaly - we have a strong indication that
 this tuple may be an informative lead for the anomaly.
 We have confirmed this hypothesis via our tests (See .......).
 
\end_layout

\begin_layout Subsection
At this stage, we have an indication for each tuple whether its corresponding
 filtered time series had an anomolous event at the inspected time window.
 If the filtered time series still contains an anomaly at the inspected
 time window - we can infer that this tuple is not a good lead to finding
 the cause for that anomaly.
 However, in the opposite case, we believe that the end-user should be able
 to inspect this tuple as a lead to the cause for the anomaly.
\end_layout

\begin_layout Subsection
ranking
\end_layout

\begin_layout Section
Data Preparation Process
\end_layout

\begin_layout Standard
We chose to focus on the Request Duaration KPI, as seen in the 
\begin_inset Quotes eld
\end_inset

requests
\begin_inset Quotes erd
\end_inset

 datasets provided by Microsoft.
 The data preprocessing can be summarized in two steps:
\end_layout

\begin_layout Subsection
Select and Preprocess data
\end_layout

\begin_layout Standard
We picked the relevant 
\begin_inset Quotes eld
\end_inset

requests
\begin_inset Quotes erd
\end_inset

 files and merge them to one csv file with all the information, filtering
 out columns that had full correlation to other column and columns that
 were very sparse (more than 80% missing values).
 We ended up with a large data set (~700K rows), where each row represents
 a request event with request duration, time stamp and some more attributes.
 Furthermore, we filtered out record with timestamps after 2015-10-07 18:00,
 since these records had insufficient amount of data points for our time-series
 analysis.
\end_layout

\begin_layout Subsection
Transform Data
\end_layout

\begin_layout Standard
As explained in the Model Workflow section, the main transformation applied
 on our datasets is aggregation by a defined time-window.
 In order to find the optimal time-window - we ran a search over tuples
 of parameters with changing time-windows and picked the best performing
 parameters, according to the f-score derived from comparing the detected
 anomalies to the labeles examples in a synthesised dataset.
 
\begin_inset Newline newline
\end_inset

In a real world application, we would have to manually define what considers
 as an anomaly in our dataset, since there can be many interpretations of
 what is an anomaly.
 Our implementation includes a process which recieves a dataset and an array
 of anomaly times and selects the optimal parameters for this dataset.
\end_layout

\begin_layout Section
Anomaly detection
\end_layout

\begin_layout Subsection
ARIMA-based Anomaly Detection
\end_layout

\begin_layout Standard
Since our workflow involves detection of anomalies in a very large amount
 of time series, our goal was to find an algorithm for anomaly detection
 which combines good accuracy and fast performance times.
 In our search for this kind of algorithm, we have tested an anomaly detection
 process based on modeling an ARIMA model with 2 different fourier terms
 (to account for the daily and weekly seasonality) and finding data points
 which are located outside of the confidence interval generated by the model.
 We also tested the function 
\begin_inset Quotes eld
\end_inset

tso
\begin_inset Quotes erd
\end_inset

 in the package 
\begin_inset Quotes eld
\end_inset

tsoutliers
\begin_inset Quotes erd
\end_inset

, that also performs an iterated version of the ARIMA fitting process by
 removing outliers in the first stage, re-fitting of the model and removal
 of the next outliers until it reaches a stopping criterion.
 
\begin_inset Newline newline
\end_inset

These two methods were tested on our datasets and although they showed good
 accuracy in detection of anomalies - their performance measurments showed
 that they were inadequate for this type of workflow.
 
\end_layout

\begin_layout Subsection
STL Decomposition Anomaly Detection
\end_layout

\begin_layout Standard
In contrast to the abovementiond methods, there exists a very accurate and
 efficient algorithm, based on STL Decomposition, found in the 
\begin_inset Quotes eld
\end_inset

forecast
\begin_inset Quotes erd
\end_inset

 package in R (tsoutliers).
\begin_inset Newline newline
\end_inset

The algorithm performs a seasonality adjustment on the time series using
 an STL decomposition and smoothing of the Trend element using Friedman's
 
\begin_inset Quotes eld
\end_inset

Super Smoother
\begin_inset Quotes erd
\end_inset

 Algorithm.
 It subtracts the 
\begin_inset Quotes eld
\end_inset

smoothed
\begin_inset Quotes erd
\end_inset

 trend from the seasonally adjusted time series and tests this new time
 series of residuals for deviations from a normal distribution, based on
 its Interquartile Ranges.
 A demonstration of this process on the 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 dataset is depicted via the following figures:
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
STL Decomposition on 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 dataset, showing the original series, the seasonal element, trend and residuals
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL-Smooth.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Seasonally-Adjusted 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 time series with the smoothed Trend line over it as the expected value
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename STL Residuals.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Residuals from the 
\begin_inset Quotes eld
\end_inset

expected value
\begin_inset Quotes erd
\end_inset

 (Smooth trend line) and the Seasonally-Adjusted time series.
 It is clear that the residuals conform to a normal distribution with mean
 0, with some outliers outside the third quartile of the distribution
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Detected Anomalies.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Plot of the Requests time series, aggregated by a 5-min time window with
 the anomalies highlighted by their severity
\end_layout

\end_inset


\end_layout

\begin_layout Section
Lead Detection
\end_layout

\begin_layout Subsection
Detecting Potential Leads for Anomalies
\end_layout

\begin_layout Standard
Throughout the process of considering different business cases to address
 in our project, we performed a survey on anomaly detection algorithms.
 We discovered that there are many processes that assist in detecting anomalies
 in time series data, but we could not find any method for pointing out
 possible causes for the anomalies seen in the data.
\begin_inset Newline newline
\end_inset

For this reason, we have decided to design a workflow that its main goal
 is to channel analysts efforts into the most probable causes of each anomaly,
 by providing a ranked list of probable leads.
 In order to provide this information in an accurate and informative manner,
 we decided to focus our attention on expanding the search space for potential
 leads, at the cost of performance times.
\begin_inset Newline newline
\end_inset

We have tested 3 different approaches for this task:
\end_layout

\begin_layout Subsubsection
Regression Tree
\end_layout

\begin_layout Standard
Given a time window labeled as an anomaly, we were able to fit a regression
 tree on the values of residuals for each record in the dataset.
 The output of the model was a tree for which each node represents a tuple
 of features along with their mean residual value.
 Its main strength is in its simplicity and efficiency - it has the ability
 to combine huge categorical variables into a short set of interpertable
 rules.
 Furthermore, it implicitly inclues variable selection and in-feature grouping
 of values.
 We used those properties and the fact that regression trees is relatively
 fast, even with large data sets, for optimization purposes.
 
\begin_inset Newline newline
\end_inset

However, this model relies on the assumption that the higher the residuals
 - the higher the probabilty of being a potential lead.
 This may not be the case in various time series.
 
\begin_inset Newline newline
\end_inset

Consider the scenario where the request duration times for US residents
 is usually lower than average and the request duration times of Russian
 residents is usually higher than average.
 If at a certain time window the servers which attend to US residents fail
 to provide fast response times - their request time will increase, but
 perhaps not above the average duration of Russian residents.
 In this case, there will be an anomaly in the time series and the Regression
 Tree will be biased towards emphasizing tuples of features including Russian
 residents instead of US residents.
 
\begin_inset Newline newline
\end_inset

For this reason, we have decided to test a different method, which relies
 on the finding anomalies based on the change in distance between subsets
 of the populations compared to the whole population, described in the following
 paragraph.
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Tree.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Example of a Regression Tree derived from anomalies in Requests data.
 Actual values for splits are abbreviated for aesthetic reasons.
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Distance-Based lead detector
\end_layout

\begin_layout Standard
In order to address the abovementioned scenario, we have implemented a method
 that once given an anomaly time window - it generates all the k-combinations
 of values that exist within the anomaly time window.
 For each such tuple we extract a time series with the absolute distance
 in values between the selected tuple-subset and the values of all the events
 excluding this tuple-subset.
 For this new vector of samples we fitted a distribution, and estimated
 the p-value of the distance at the anomaly point.
 In the case where there was an anomaly within the distance time series
 at the same time of the original anomaly time - we would have a strong
 indication whether this tuple of features is an informative lead for the
 anomaly.
\begin_inset Newline newline
\end_inset

Combining the two complementary methods described above into one general
 model would probably cover many of the different scenarios possible in
 real world applications.
 However, after testing out different implementations of combining these
 two methods, we realized that it is hard to generalize the impact of each
 element in the overall model.
 Moreover, the distance-based lead detector must rely on a sufficient amount
 of records in order to perform significance testing, which might not be
 available for sparse tuples which may be very anomolous and, for our case,
 informative.
 
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename C:/R Workspace/Anomaly-Lead-Extraction/Distances.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
An example of a time series of distances between a subset of data and the
 complementary time series.
 Notice the anomaly at the points 87 and 88 which show an anomaly with regards
 to the previous distribution of distances
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Multiple Time-Series Anomaly Detection
\end_layout

\begin_layout Standard
Finally, we have decided to focus on a method which, from our observations,
 is general enough to detect various types of anomalies and leads in an
 elegant and intelligible way.
\begin_inset Newline newline
\end_inset

Our selected method, as described in the Model Workflow section, relies
 on the understanding that if we remove records from a subset of the data
 (derived from a tuple of features) and we observe that the filtered time
 series shows no anomaly in the time window where we originally detected
 it, we can assume that it is related, in some sense to the cause of the
 anomaly.
 If, in addition to that, we find that the filtered time series is very
 similar in its values and properties to the original time series - we can
 asses the degree to which this subset of data can direct us to the cause
 for the anomaly.
 
\begin_inset Newline newline
\end_inset

In order to carry out this method, we extract all the 3-tuple combinations
 in the anomolous time-window and find the tuples which induce a filtered
 time series that shows no anomaly at that time window.
\begin_inset Newline newline
\end_inset

Because there might be several tuples that we believe are potential leads,
 we rank and sort them using 3 measures attained from our algorithm:
\begin_inset Newline newline
\end_inset


\series bold

\begin_inset Quotes eld
\end_inset

New Distance
\begin_inset Quotes erd
\end_inset


\series default
 - what is the absolute distance in standard deviations at the anomaly time
 window between the expected value generated in the anomaly detection algorithm
 and the actual value of the time series.
 The lower the score, the stronger the indication that this tuple is an
 informative lead.
\begin_inset Newline newline
\end_inset


\series bold

\begin_inset Quotes eld
\end_inset

Unique Anomalies
\begin_inset Quotes erd
\end_inset


\series default
 measure - the number of anomalies found in the filtered time series that
 didn't appear in the original time series.
 We use this metric in order to as an efficient indicator for the dissimalirity
 between the two time series.
 The lower the number, the higher the probability that the two time series
 are similar.
\begin_inset Newline newline
\end_inset


\series bold

\begin_inset Quotes eld
\end_inset

Overall Anomalies
\begin_inset Quotes erd
\end_inset


\series default
 measure - the percentage of anomalies that the filtered time series agrees
 with the original time series.
 This is another indication of the similarity between the time series, but
 for lower numbers - it can tell us that the inspected lead is a potential
 cause for many anomalies in other time points.
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Tuples-Scores.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
A snippet of the data table generated from the Multiple Time-Series anomaly
 detection process on the 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 data
\end_layout

\end_inset


\begin_inset Newline newline
\end_inset

After obtaining the scores for all the tuples that show no anomaly at the
 specified time, we cluster them using Ward's method according to both metrics,
 after standartization: 
\begin_inset Quotes eld
\end_inset

New Distance
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

Unique Anomalies
\begin_inset Quotes erd
\end_inset

.
 After our experiments we concluded that combining these two features as
 the main ranking scheme provides a good estimate of the effect each tuple
 had on the anomaly.
\begin_inset Newline newline
\end_inset

At the next step, we sort the clusters by their proximity to the origin
 (0,0), beacuse the lower both scores are the greater the probability for
 the tuple to be a good lead for the anomaly.
 
\begin_inset Newline newline
\end_inset

Finally, we present to the end user a list which consist of all tuples,
 ordered by the cluster distance from the origin and with a secondary sorting
 using the third score: 
\begin_inset Quotes eld
\end_inset

Overall Anomalies
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Section
Synthetic Data Set
\end_layout

\begin_layout Standard
In order to estimate our performance and validate our assumptions, we decided
 to synthesize a labeled dataset which preserves our original 
\begin_inset Quotes eld
\end_inset

Requests
\begin_inset Quotes erd
\end_inset

 dataset properties in terms of seasonality, trend, overall values and deviation
s inside hourly time-windows.
 
\end_layout

\begin_layout Subsection
Creating the base of the data set
\end_layout

\begin_layout Standard
In our attempt to preserve the main properties of our data, we first trimmed
 the events with request duration value greater than 
\begin_inset Formula $\pm$
\end_inset

3 standard deviations from the mean.
 This gave us a cleaner version of the original dataset, without extreme
 outliers.
 
\begin_inset Newline newline
\end_inset

In order to emulate the seasonality and trend, we calculated the average
 request duration per day and per hour.
 Furthermore, we calculated the ratio of the average request duration of
 an hour and average request duration of a day, assuming that this ratio
 is similiar for each hour and day (based on the seasonality we have seen
 in our data).
 
\begin_inset Newline newline
\end_inset

For each hour we created approximately 4000 random time stamps, which is
 derived from the number of events per hour on average, then attached to
 it request duration values that were generated from normal distribution
 with proper mean and standard deviation for this hour.
 We added to this data frame another 4 columns which represents our attributes.
 Each coulmn values were generated from different discrete distributions.
 The choice of discrete distributions is owing to the fact that our original
 features are categorical.
 1 attribute was chosen from discrete uniform distribution, 2 attributes
 from (different) binomial distributions and another attribute from poisson
 distribution.
 
\end_layout

\begin_layout Subsection
Inserting anomalies
\end_layout

\begin_layout Standard
At this point we want to inject artificial anomalies in random time stamps,
 and randomly choose how many features and which features to be the cause
 of the anomaly.
 In addition, we generated randomly a value which is the strength of the
 anomaly.
 That way, we will have different kind of anomalies (global and seasonal).
 For each event in the anomaly time range that contains the 
\begin_inset Quotes eld
\end_inset

problematic
\begin_inset Quotes erd
\end_inset

 features, we increase the request duration - add to its value the multiplicatio
n of the strengh of the anomaly with the standard deviation that was calculated
 the previous stage.
 We recored the properties of each anomaly so we can measure our result
 on the synthetic data.
 Eventually we exported our synthetic data frame to csv file.
 Illustration of the synthetic data with anomalies:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename synthetic.png
	lyxscale 70
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
A plot of the synthetic time series
\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Labled Anomalies.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
A Snippet of labeled anomalies in the synthetic dataset
\end_layout

\end_inset


\end_layout

\begin_layout Section
Testing
\end_layout

\begin_layout Standard
The synthetic dataset contains two accompanying datasets: 1.
 Anomaly timestamp and 2.
 Tuples of features that generated each anomaly
\end_layout

\begin_layout Subsection
Anomaly detection accuracy
\end_layout

\begin_layout Standard
We measured the accuracy of the anomaly detection process using F-score.
 It considers both the precision and the recall and can be interpreted as
 the harmonic mean of this two scores.
 F-score reaches its best value at 1 and worst at 0.
 
\begin_inset Newline newline
\end_inset

To explain our calculation we first define the following parameters:
\begin_inset Newline newline
\end_inset

TP (true positive) = #anomalies correctly labeled as anomalies 
\begin_inset Newline newline
\end_inset

FP (false positive) = #anomalies incorrectly labled as anomalies 
\begin_inset Newline newline
\end_inset

FN (false negative) = #anomalies that were not labeled as anomalies
\begin_inset Newline newline
\end_inset

Precision = 
\begin_inset Formula $\frac{tp}{tp+fp}$
\end_inset

 , meaning, from all the anomalies our model detected what is the percentage
 we were right
\begin_inset Newline newline
\end_inset

Recall = 
\begin_inset Formula $\frac{tp}{tp+fn}$
\end_inset

 , meaning from all the anomalies actuall exist in the data what is the
 prcentage that we detected
\begin_inset Newline newline
\end_inset

F-score = 
\begin_inset Formula $2*\frac{precision*recall}{precision+recall}$
\end_inset


\begin_inset Newline newline
\end_inset

We were able to achive an impressive f-score of 
\series bold
0.98
\series default
 on our synthetic dataset with 0.22 ms processing time on 700K records dataset.
 The optimal parameteres were: 
\series bold
Window size = 8
\series default
, 
\series bold
IQR factor = 4
\series default
, 
\series bold
Iterations = 2
\series default
.
 The last two parameters are inputs to our anomaly detection procedure.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $Plot$
\end_inset


\end_layout

\begin_layout Subsection
Lead Detection Accuracy
\end_layout

\begin_layout Standard
In order to validate our assumptions on detection of informative leads in
 a time series, we chose to depict our accuracy as a function of the number
 of results (k) presented to the end-user.
 
\begin_inset Newline newline
\end_inset

The accuracy measured in this step is the precentage of correct tuples found
 in the top ranked k tuples provided by our model.
\begin_inset Newline newline
\end_inset

For example, given k = 5, we evaluate the percentage of the correct tuples
 found in the top 5 ranked tuples in all of the detected anomalies.
 Our experimentation showed high accuracy (92%) attained from k=4 and 100%
 accuracy at k=7.
 We also evaluated a model which chooses tuples at random and presented
 its accuracies as well.
\begin_inset Newline newline
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename C:/R Workspace/Anomaly-Lead-Extraction/Model Accuracy Graph.png

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
Accuracy measures as a function of k-top results.
 Red line shows accuracy by a random tuple selection process
\end_layout

\end_inset


\end_layout

\begin_layout Section
Appendix
\end_layout

\begin_layout Subsection
Cluster subsets
\end_layout

\begin_layout Standard
We used the Ward's method to clusetr the subsets by their score.
 The aim in Ward’s method is to join cases into clusters such that the variance
 within a cluster is minimised.
 To do this, each case begins as its own cluster.
 Clusters are then merged in such a way as to reduce the variability within
 a cluster.
 To be more precise, two clusters are merged if this merger results in the
 minimum increase in the error sum of squares.
 Basically, this means that at each stage the average similarity of the
 cluster is measured.
 The difference between each cases within a cluster and that average similarity
 is calculated and squared (just like calculating a standard deviation).
 The sum of squared deviations is used as a measure of error within a cluster.
 A case is selected to enter the cluster if it is the case whose inclusion
 in the cluster produces the least increase in the error (as measured by
 the sum of squared deviations).
 No apriori information about the number of clusters required.
 We choose the closest cluster to the origin and take the subsets which
 correspond to the cluster cases as mentioned in section 2.
\end_layout

\begin_layout Subsection
p-value calculation
\end_layout

\begin_layout Subsubsection
Fitting 
\series bold
Distribution to data sample:
\end_layout

\begin_layout Standard
\noindent
Fitting distributions consists in finding a mathematical function which
 represents in a good way a statistical variable.
 For some observations of a quantitative character 
\begin_inset Formula $x_{1},...,x_{n}$
\end_inset

 we wishes to test if those observations, being a sample of an unknown populatio
n, belong from a population with a pdf (probability density function) f(x,θ),
 where θ is a vector of parameters to estimate from available data.
 
\begin_inset Newline newline
\end_inset

We can identify 3 steps in fitting distributions:
\end_layout

\begin_layout Enumerate
Model/function choice: hypothesize families of distributions; 
\end_layout

\begin_layout Enumerate
Estimate parameters;
\end_layout

\begin_layout Enumerate
Goodness of fit statistical tests.
\end_layout

\begin_layout Subsubsection
Model choice:
\end_layout

\begin_layout Standard
On the paper Probabilistic approaches to risk by Aswath Damodaran.
 In Appendix 6.1 Aswath discusses the key characteristics of the most common
 distributions and in Figure 6A.15 he provides us with a decision tree diagram
 for choosing a distribution:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Graphics
	filename distributions.png
	scale 70

\end_inset


\begin_inset Newline newline
\end_inset

Our statistic representing the distance from the predictive request duration,
 thus, is continuos 'not only positive and there are no limits on the data.
 therefoe, we will choose all the continuos distrubutions except the Exponential
 and the Triangular as our hypothesize families of distributions.
\end_layout

\begin_layout Subsubsection
Estimate parameters:
\end_layout

\begin_layout Standard
After choosing the models that can mathematically represent our data we
 have to estimate parameters of such model.
 There are several estimate methods in statistical literature, we focused,
 as we sdudied in statistic course, on maximum likelihood:
\begin_inset Newline newline
\end_inset

We have a random variable with a known pdf f(x,θ) describing a quantitative
 character in the population.
 We should estimate the vector of costant and unknown parameters θ according
 to sampling data: 
\series bold

\begin_inset Formula $x_{1},x_{2},...,x_{n}$
\end_inset


\series default
.
 
\begin_inset Newline newline
\end_inset

Maximum likelihood estimation begins with the mathematical expression known
 as a likelihood function of the sample data.
 Loosely speaking, the likelihood of a set of data is the probability of
 obtaining that particular set of data given the chosen probability model.
 This expression contains the unknown parameters.
 Those values of the parameter that maximize the sample likelihood are known
 as the maximum likelihood estimates (MLE).
 
\begin_inset Newline newline
\end_inset

We define the likelihood function as: 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)=\prod_{i=1}^{n}f(x_{i},\vartheta)$
\end_inset

 MLE consist in finding θ which maximizes 
\begin_inset Formula $L(x_{1},x_{2},..,x_{n},\vartheta)$
\end_inset

 or its logarithmic function.
\begin_inset Newline newline
\end_inset

We can employ mathematical analysis methods (partial derivates equal to
 zero) when the likelihood function is rather simple, but very often we
 should had using iterative methods.
 therefore, we implemented using R method fitdistr() - included in package
 MASS for maximum-likelihood fitting of univariate distributions without
 any information about likelihood analytical expression.
 It is enough to specify a data vector, the type of pdf and eventually the
 list of starting values for iterative procedure.
\end_layout

\begin_layout Subsubsection
Goodness of fit tests:
\end_layout

\begin_layout Standard
Goodness of fit tests indicate whether or not it is reasonable to assume
 that a random sample comes from a specific distribution.
 They are a form of hypothesis testing where the null and alternative hypotheses
 are:
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{0}$
\end_inset

: Sample data come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Formula $H_{A}$
\end_inset

: Sample data do not come from the stated distribution
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

These tests are sometimes called as omnibus test and they are distribution
 free, meaning they do not depend according the pdf.
\begin_inset Newline newline
\end_inset

We chose using the Kolmogorov-Smirnov test:
\begin_inset Newline newline
\end_inset

This test is used to decide if a sample comes from a population with a specific
 distribution.
\begin_inset Newline newline
\end_inset

It is restricted to continuous distributions and based on a comparison between
 the empirical distribution function (ECDF) and the theoretical one defined
 as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F(x)=\intop_{a}^{x}f(y,\vartheta)dy$
\end_inset

 , where f(y,θ) is the pdf.
\begin_inset Newline newline
\end_inset

Given n ordered data points 
\begin_inset Formula $x_{1},x_{2},...x_{n}$
\end_inset

 , the ECDF is defined as:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $F_{n}(X_{i})=N(i)/n$
\end_inset

 where N(i) is the number of points less than 
\begin_inset Formula $X_{i}$
\end_inset

(
\begin_inset Formula $X_{i}$
\end_inset

are ordered from smallest to largest value).
 This is a step function that increases by 1/n at the value of each ordered
 data point.
\begin_inset Newline newline
\end_inset

The test statistic used is:
\begin_inset Newline newline
\end_inset


\begin_inset Formula $D_{n}=sup_{1\leq i\leq n}|F(X_{i})-F_{n}(X_{i})|$
\end_inset

 that is the upper extreme among absolute value differencies between ECDF
 and theoretical CDF.
\begin_inset Newline newline
\end_inset

The hypothesis regarding the distributional form is rejected if the test
 statistic, 
\begin_inset Formula $D_{n}$
\end_inset

, is greater than the critical value obtained from a table, or, which is
 the same, if the p-value is lower than the significance level.
\begin_inset Newline newline
\end_inset


\begin_inset Newline newline
\end_inset

Another test usually taken into consideration is the chi-square goodness
 of fit test.
 But it is applied to binned data, namely data that have been split into
 classes.
 Unfortunately, this test depends consistently on how the data have been
 binned.
\begin_inset Newline newline
\end_inset

The Kolmogorov-Smirnov test is more powerful than the chi-square test in
 the cases in which the sample size is, so to say, unreliable by the presence
 of noise, missing values, etc.
 For large sample sizes both the tests have the same power.
 One limitation of the Kolmogorov-Smirnov test is that the distribution
 needs to be fully specified.
 Therefore the shape and its estimated parameters must be computed beforehand.
 That is exactly what our code does.
\end_layout

\begin_layout Subsubsection
R procedure
\end_layout

\begin_layout Standard
The procedure parameters are the data (distances samples) and the distance
 of the request duration of a specific features subset event from the predictive
 request duration at the anomaly point .
 The purpose of the procedure is to select the best shape and the best parameter
s for the distributions mentioned above and perform a significance test
 of that selection as explained.
 The goal here cannot be to determine with certainty what distribution our
 sample follows.
 The goal is what calls parsimonious approximate descriptions of the data.
 We measure the P-Value of the given distance according to the distditribution
 which explain our data most.
 and return it for determine if to use that subset in the next stage.
 (the procedure is in find_dist.R)
\end_layout

\end_body
\end_document
